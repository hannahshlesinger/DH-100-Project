{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rupi Kaur.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMPBH0DKhaHTix9DkaHEnNL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"00z3ivyacWoj"},"source":["# Text Analysis of Rupi Kaur's Poetry\n","Hannah Shlesinger\n","\n","hcs@berkeley.edu\n","\n","Instructor: Dr Adam Anderson\n","\n","06 June 2021\n"]},{"cell_type":"markdown","metadata":{"id":"1ITB1OYeDmt5"},"source":["**Note:** I'm starting with just milk and honey and I will apply similar processes to the other texts and data as I go along, but I wanted to keep it to a single document to begin with just while I'm getting familiar with the process\n"]},{"cell_type":"markdown","metadata":{"id":"rCf3g62WcQsr"},"source":["#Imports and Installations\n","\n","Importing relevant libraries for analysis \n","Using DH101 Jupytr Notebook 2-1 as a model \n","\n","I'm keeping all installations and imports at the top to make it easier to refer back to them\n","\n","**Note: **I'm pretty sure I installed / imported some I won't end up using in the end, but they're all here for now"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZUIVJQ6f3QX","executionInfo":{"status":"ok","timestamp":1623028226880,"user_tz":420,"elapsed":164407,"user":{"displayName":"Hannah Shlesinger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmrzlECe_AAtxdkEar6bMq4YpssTNdMiOawlo3=s64","userId":"16280862987021110564"}},"outputId":"29c1a43f-8c9e-44f8-9899-82e944dbda9c"},"source":["\n","!pip install spacy\n","#spaCy is an open-source software library for advanced natural language processing\n","\n","!python -m spacy download en_core_web_sm \n","#This downloads a small trained English model \n","\n","!python -m spacy download en_core_web_lg\n","#this downloads a large trained English model\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n","Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Collecting en_core_web_lg==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n","\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.0.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180945 sha256=041d34456675511a602a3eb404acfc642545a25496adf0db389359b34b440549\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-l5b5ykzg/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jnXhqZ--hoHa","executionInfo":{"status":"ok","timestamp":1623028228208,"user_tz":420,"elapsed":1337,"user":{"displayName":"Hannah Shlesinger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmrzlECe_AAtxdkEar6bMq4YpssTNdMiOawlo3=s64","userId":"16280862987021110564"}}},"source":["import pandas as pd\n","import numpy as np\n","import spacy\n","import re\n","from string import punctuation\n","nlp = spacy.load('en_core_web_sm')\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRiRSgiFhyyW","executionInfo":{"status":"ok","timestamp":1623028228650,"user_tz":420,"elapsed":444,"user":{"displayName":"Hannah Shlesinger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmrzlECe_AAtxdkEar6bMq4YpssTNdMiOawlo3=s64","userId":"16280862987021110564"}}},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('seaborn-bright')\n","import seaborn as sns"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AsocBrvk8wR"},"source":["Module to help remove stop words"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FImG6Cw3gRJR","executionInfo":{"status":"ok","timestamp":1623028230236,"user_tz":420,"elapsed":1589,"user":{"displayName":"Hannah Shlesinger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmrzlECe_AAtxdkEar6bMq4YpssTNdMiOawlo3=s64","userId":"16280862987021110564"}},"outputId":"ca1b9d6b-43d0-4f8d-816c-8d27560d880e"},"source":["\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"punkt\")\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QnXQD47nk5r_"},"source":["Module to count word frequencies"]},{"cell_type":"code","metadata":{"id":"yzdaNdQ6gRaP","executionInfo":{"status":"ok","timestamp":1623028230237,"user_tz":420,"elapsed":6,"user":{"displayName":"Hannah Shlesinger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmrzlECe_AAtxdkEar6bMq4YpssTNdMiOawlo3=s64","userId":"16280862987021110564"}}},"source":["from collections import Counter\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTH1ZSADk2NC"},"source":["Mount to Google Drive\n"]},{"cell_type":"code","metadata":{"id":"kt1jlbqnk4JC"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXLdjKoeIREB"},"source":["#Read in Pandas \n","Converting text to be read in a pandas dataframe"]},{"cell_type":"markdown","metadata":{"id":"f1IFh3UaPepu"},"source":["**Note:** I converted the text into a .csv file in OverviewDocs\n","\n","I'm not sure this was the best method\n","\n","It divided the text by page which is useful because each poem is usually split by page\n","\n"]},{"cell_type":"code","metadata":{"id":"NlgUnfsaJxym"},"source":["df_milkhoney = pd.read_csv(\"/content/drive/MyDrive/Hannah Shlesinger/Jupytr /milk-and-honey.csv\")\n","df_milkhoney.head(20)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vv6oPiZv4YEY"},"source":["#Tokenization \n","I'm using the DH101 2-1 Juptyr Notebook to breakdown the text into parts"]},{"cell_type":"code","metadata":{"id":"duBV1LPfQ0t4"},"source":["milkandhoney = open(\"/content/drive/MyDrive/Hannah Shlesinger/Jupytr /milk-and-honey.txt\").read()\n","#print(milkandhoney)\n","#uncomment to view full text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QqCc9hN2WBns"},"source":["***From here, I used [Christina's notebook](https://drive.google.com/file/d/1jvlUrIsxkvNo-15ch9H0Sw5PKGTPGW1Y/view?usp=sharing) as a model to break down my txt file into a file without punctuation ***"]},{"cell_type":"markdown","metadata":{"id":"t69VHnq0WaZi"},"source":[".readlines() creates a list from the text that breaks down each line into an element"]},{"cell_type":"code","metadata":{"id":"_qt2QptCTXwK"},"source":["with open (\"/content/drive/MyDrive/Hannah Shlesinger/Jupytr /milk-and-honey.txt\", \"r\") as myfile:\n","    data1=myfile.readlines()\n","    print(data1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwUGntEYWmtf"},"source":["generates the list into a single string"]},{"cell_type":"code","metadata":{"id":"km6n5qtCTw5m"},"source":["part1 = ''.join(data1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IWjPiQXWq1w"},"source":["NLTK's sent_tokenize function to split each sentence into its own element into a list -- uncomment last line to view a list where each element is a sentence\n","\n","Note: may contain '\\n', signaling where a new line should start  --> deal with later"]},{"cell_type":"code","metadata":{"id":"9GBTHSApT-VY"},"source":["sent_part1 = sent_tokenize(part1) \n","part1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPhbq_xUW9Wm"},"source":["takes out line breaks ('\\n')"]},{"cell_type":"code","metadata":{"id":"mSyGMAvUU4Pa"},"source":["sent_part1 = [sent.replace(\"\\n\", \" \") for sent in sent_part1] \n","#sent_part1\n","#uncomment to view"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80qUqJwIXHuE"},"source":["Gets rid of punctuation"]},{"cell_type":"code","metadata":{"id":"HzQr5vD6VLLZ"},"source":["sent_part1_no_punc = [re.sub(r\"[^\\w\\s]\", \"\", sent) for sent in sent_part1] \n","#sent_part1_no_punc\n","#uncomment to view"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqOlWwe5fhmN"},"source":["**Note:** This is where my reference of Christina's notebook pauses. I need to look a bit more into what she's doing, but up until here was useful to breakdown my own text file. "]},{"cell_type":"markdown","metadata":{"id":"_MUaW0Hnfq2h"},"source":["#Lemmatization\n","lemmatization breaks down words into their dictionary roots \n","\n","*process modeled after DH101 2-1 text analysis Jupytr Notebook*"]},{"cell_type":"markdown","metadata":{"id":"uDIEDqTJf5kQ"},"source":["defining lemmatization function"]},{"cell_type":"code","metadata":{"id":"QJNL9lOBXk1Q"},"source":["def lemmatize(tokens):\n","    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n","    words = ' '.join(tokens)\n","    doc = nlp(words)\n","    return [token.lemma_ for token in doc]\n","\n","tokens = (part1).split()\n","\n","lemmas = lemmatize(tokens)\n","print(lemmas)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfhKuzoJgDSP"},"source":["joins tokens back together as a string so it can be converted into a dataframe identifying the different parts of speech\n"]},{"cell_type":"markdown","metadata":{"id":"exr8emV-Ahw9"},"source":["defining function and data frame"]},{"cell_type":"code","metadata":{"id":"RMX2Wh9He1CZ"},"source":["doc = nlp(\" \".join(tokens))\n","\n","d = []\n","for token in doc:\n","    d.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n","          token.shape_, token.is_alpha, token.is_stop))\n","\n","out = pd.DataFrame(d, columns=(\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"shape\", \n","                               \"is_alpha\", \"is_stop\"))\n","out.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9xBuqgYgMbJ"},"source":["**[Issue]**: I don't yet know how to get rid of the 'additional' text in the book such as the copyright information. I think it will be a matter of selecting a range of rows from when the poems start to the end of the poems, but for now I have all of it lumped together"]},{"cell_type":"markdown","metadata":{"id":"yOFoUqtHgbDO"},"source":["#Counting Frequent Words\n","Using the counter function, I am calculating the most commonly used words in the text"]},{"cell_type":"code","metadata":{"id":"dQE2q2n3e8su"},"source":["milk_freq = Counter(tokens)\n","milk_freq.most_common(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2e8d26hLgjIY"},"source":["I'm using stopwords to eliminate common english words "]},{"cell_type":"code","metadata":{"id":"kxhtCDD6fH5y"},"source":["stop = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nusg7JO7go86"},"source":["here is my text without the stop words"]},{"cell_type":"code","metadata":{"id":"oXXKleN1fPKR"},"source":["no_stops = [word for word in tokens if word not in stopwords.words('english')]\n","print(no_stops)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQeRL3fGgs-7"},"source":["now, when I count the frequency of words, it gives me only the 'unique' words"]},{"cell_type":"code","metadata":{"id":"OUp3AoDAfQv6"},"source":["milk_freq2 = Counter(no_stops)\n","mh_barplot = milk_freq2.most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6qbDr39hcvz"},"source":["#Visualizations\n","creating a visualization of word frequency"]},{"cell_type":"code","metadata":{"id":"MCLX3VvUhkK8"},"source":["milkhoney_df = pd.DataFrame(data = mh_barplot, \n","                         columns = [\"Word\", \"Frequency\"])\n","milkhoney_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plOjObwmkQvL"},"source":["milkhoney_df.plot.bar(x = \"Word\", y = \"Frequency\", figsize = (8,5))\n","plt.xlabel(\"Word (not including stopwords)\")\n","plt.ylabel(\"Frequency\")\n","plt.title(\"Bar plot of most common words in milk and honey by Rupi Kaur\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_gWPAOVQQBf"},"source":["# **Next Steps**\n","Figuring out n-gram models\n","\n","Classification model\n","\n","other visualizations for comparison\n","\n","topic /network modeling\n","\n","importing the other books (*the sun and her flowers* and *homebody*)\n","and repeat process\n","\n","Identify / fix issues with how the data is being broken up (I want to be able to divide between poems potentially...)\n"]}]}